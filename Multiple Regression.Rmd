---
title: "Multiple Regression"
author: "Deviprasad Saka"
date: "2024-04-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
file_path <- "D:/Multivariate Analysis/Assignment-7/Ship_data.csv"
mydata <- read.csv(file_path)
autompg <- read.csv("D:/Multivariate Analysis/Assignment-7/Ship_data.csv")
str(mydata)
```




 Q1.Model Development
```{r}
# Performing multiple regression on the dataset
# Fit a linear regression model
fit <- lm(Power ~ V + n + tot_r + t + w, data = mydata)
#show the results
summary(fit)
```

This summary provides the results of a multiple linear regression model where the response variable is "Power" and the predictor variables are "V," "n," "tot_r," "t," and "w.".
Overall, the model seems to have very high explanatory power, with an adjusted R-squared of 1, indicating that all variability in the "Power" variable is explained by the predictors. However, it's important to interpret the coefficients carefully, considering the significance levels and the practical implications of each predictor.

In this step, we loaded the ship_data and fitted a multiple regression model using the lm() function. The model predicts the power based on several predictor variables: Ship speed(V), Propeller
rps(n), Total resistance (tot_r), Thrust deduction fraction (t) and wake fraction (w).




Q2: Model Acceptance
From the summary among the predictors included in the model, "n" and "tot_r" have stronger correlations with "Power" compared to "V," "t," and "w.". The coefficient estimate for "tot_r" is 3.312e+01, and the associated p-value is <2e-16, indicating high statistical significance. Therefore, "tot_r" has a strong positive correlation with "Power.". The coefficient estimate for "n" is -2.649e+04. The associated p-value is <2e-16, indicating high statistical significance. Therefore, "n" has a strong negative correlation with "Power.".

```{r}
coefficients(fit)
```


These are the estimated coefficients for each predictor variable in the multiple linear regression model:

Intercept: The estimated intercept of the regression line is approximately 35493.006.
  
V: The estimated coefficient for the predictor variable "V" is approximately 1.209.
  
n: The estimated coefficient for the predictor variable "n" is approximately -26492.202.
  
tot_r: The estimated coefficient for the predictor variable "tot_r" is approximately 33.116.
  
t: The estimated coefficient for the predictor variable "t" is approximately -16.340.
  
w: The estimated coefficient for the predictor variable "w" is approximately -4.299.

These coefficients represent the change in the response variable "Power" for each unit increase in the corresponding predictor variable, holding all other variables constant. For example, for each unit increase in "V," the estimated change in "Power" is approximately 1.209, and so on for the other variables.

```{r}
confint(fit,level=0.95)
```

The 95% confidence intervals indicate the range of plausible values for each coefficient estimate in the multiple linear regression model. For instance, we are 95% confident that the true population coefficient for "n" falls within the range of approximately -28787.725 to -24196.678, based on the sample data.


```{r}
fitted(fit)
```

Q3.Residual Analysis

```{r}
library(GGally)
```
```{r}
ggpairs(data=mydata, title="Ship_data")
```

```{r}
plot(fit, which=1) # Residuals vs Fitted
plot(fit, which=2) # Normal Q-Q plot

```

This is a Residuals vs Fitted plot, which is commonly used in regression analysis to assess the goodness of fit of a regression model. The main components of the graph are as follows:

Horizontal Axis (X-axis): This represents the fitted values, which are the predicted values generated by the regression model. These values range from approximately 16,000 to 22,000.

Vertical Axis (Y-axis): This represents the residuals, which are the differences between the observed values and the fitted values from the model. The residuals range from about -30 to 20.

Points on the Graph: Each point represents an observation from the dataset. The position of the point is determined by its fitted value and its residual.

Smooth Red Line: This line is a smooth curve that helps to visualize the trend in the residuals. It appears to show a non-linear pattern, suggesting that the relationship between the predictors and the response variable may not be perfectly linear.

Equation on the X-axis: The equation "lm(Power ~ V + n + tot_r + t + w)" suggests that the linear model (lm) has been fitted using 'Power' as the response variable and 'V', 'n', 'tot_r', 't', and 'w' as predictor variables.

From the pattern of the points and the red line, it looks like there may be some non-linearity in the relationship between the predictors and the response variable, or there might be other issues such as heteroscedasticity (non-constant variance of residuals) or outliers affecting the model's performance. Ideally, for a well-fitted linear regression model, the residuals should be randomly scattered around the horizontal axis (zero), with no discernible pattern. The presence of a pattern might indicate that the model is not capturing all the nuances of the data, and further investigation or a different type of model might be necessary.



In the Q-Q plot the points roughly follow the dotted line but start to deviate in the tails, especially in the right tail (positive theoretical quantiles). This suggests that the residuals are approximately normally distributed, but there may be some issues with heavier tails than expected in a normal distribution, indicating possible outliers or that the model may not fully capture all the predictive variability.It(Q-Q plot) is a tool for determining if a set of data plausibly came from some theoretical distribution such as a Normal, t, or Chi-squared distribution. For a good model fit, you'd expect the points to fall on or very close to the dotted line.



```{r}
residuals <- residuals(fit)
residuals
```

```{r}
#Plot residuals against fitted values to check for homoscedasticity
plot_resid_fitted <- ggplot() +
  geom_point(aes(x = fitted(fit), y = residuals)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "blue") +
  labs(x = "Fitted Values", y = "Residuals",
       title = "Residuals vs Fitted Values Plot") +
  theme_minimal()
print(plot_resid_fitted)

```


Q4.Prediction

```{r}
new_data <- data.frame(predict.lm(fit, data.frame(V = 10,n = 2,t = 0.2,tot_r = 1300, w = 0.25)))
new_data
```


Here when random values are given to the independent variables Ship speed(V), Propeller
rps(n), Total resistance (tot_r), Thrust deduction fraction (t) and wake fraction (w). the model predicted the dependent variable power would be 25566.97.


Q5.Model Accuracy
```{r}
#Make predictions using the model
predicted <- predict(fit, newdata = mydata)
```

```{r}

# Generate new data for prediction
new_data <- data.frame(V = 10, n = 2, t = 0.2, tot_r = 1300, w = 0.25)

# Make predictions
predicted_ship_data <- predict(fit, newdata = new_data)
print(predicted_ship_data)

# Model Accuracy
rsquared <- summary(fit)$r.squared
cat("R-squared:", rsquared, "\n")

adjusted_rsquared <- summary(fit)$adj.r.squared
cat("Adjusted R-squared:", adjusted_rsquared, "\n")

```



The high R-squared value of 0.9999651 and adjusted R-squared value of 0.9999633 suggest that the model explains almost all of the variability in the response variable, indicating an excellent fit to the data.


Visualizations plots:

```{r}
library(car)
#Nonlinearity
# component + residual plot
crPlots(fit)
```

These plots represents a comprehensive set of diagnostic plots for a regression analysis. The "Component + Residual Plots" provide insights into the relationships between the predictor variables and the response. The top-right plot shows the fitted values versus the actual values, indicating the overall goodness of fit. The bottom plots examine the residuals, checking for patterns that could violate the assumptions of the regression model, such as linearity and homoscedasticity. The scatter of points appears to be randomly distributed around the zero line, suggesting the model is adequately capturing the underlying relationships in the data. These plots collectively allow the analyst to thoroughly evaluate the validity and performance of the regression model.

```{r}
# plot studentized residuals vs. fitted values
library(car)
spreadLevelPlot(fit)
```
This Spread-Level Plot is a diagnostic tool used in regression analysis to evaluate the homoscedasticity assumption. The x-axis represents the fitted values, while the y-axis shows the absolute standardized residuals. Ideally, the residual spread should be constant across fitted values, but here it decreases as fitted values increase, forming a funnel-like shape. This suggests the presence of heteroscedasticity, where the residual variance is not constant. The dashed lines represent the fitted values for the residual spread, allowing assessment of the heteroscedasticity pattern. This plot provides a visual way to identify potential violations of the homoscedasticity assumption, which may require further investigation or the use of appropriate modeling techniques to address the issue.

