---
title: "Socialmedia"
author: "Deviprasad Saka"
date: "2024-03-28"
output: html_document
---




```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE)
```

```{r}
sm <- read.csv("C:/Users/sakad/Downloads/social_media_cleaned.csv")
str(sm)

```


```{r}
sm1 <- sm[, 2:9]
sm1
```
#PCA

```{r}
#Get the correlations between the variables 

cor(sm1, use = "complete.obs")
```
```{r}
#Computing Principal Components
social_pca <- prcomp(sm1,scale=TRUE)
social_pca

```

```{r}
summary(social_pca)
```
```{r}
eigen_social<- social_pca$sdev^2
eigen_social
```

#From PCA variate representation of each PC, It's evident that PC1 and PC2 add arround 50% of the to total variance.

#Screeplot
```{r}
plot(eigen_social, xlab = "Component number", ylab = "Component variance", type = "l", main = "Scree diagram")
```
```{r}
plot(log(eigen_social), xlab = "Component number", ylab = "Component variance", type = "l", main = "Scree diagram")
```


#From the screeplot elbow it is benefial to consider PC1,PC2,PC3,PC4,PC5 as it covers 84% of total variance.

#Visualization using PC's
```{r}
library(FactoMineR)
library("factoextra")
res.pca <- PCA(sm1, graph = FALSE)
fviz_pca_var(res.pca, col.var = "black")
```




#Factor Analysis
```{r}
# load library for factor analysis
library(ggplot2)
library(psych)

```

#Decide how many Factors are ideal for your dataset?
```{r}
fa.parallel(sm1)
```
#Parallel analysis suggests that the number of factors =  0  and the number of components =  0


#Explain the output for your factor model?
```{r}
fit.pc <- principal(sm1, nfactors=2, rotate="varimax")
fit.pc
```

#High absolute values (close to 1) indicate a strong relationship between the variable and the factor.
#h2 explains how much variance of the variables are explained by the factors.
#u2 indicates the amount of variance not explained by the factors
Principal Components Analysis
Call: principal(r = sm1, nfactors = 2, rotate = "varimax")
Standardized loadings (pattern matrix) based upon correlation matrix

                       RC1  RC2
SS loadings           2.27 1.80
Proportion Var        0.25 0.20
Cumulative Var        0.25 0.45
Proportion Explained  0.56 0.44
Cumulative Proportion 0.56 1.00

Mean item complexity =  1.3
Test of the hypothesis that 2 components are sufficient.

The root mean square of the residuals (RMSR) is  0.14 
 with the empirical chi square  29.01  with prob <  0.066 
 

```{r}
round(fit.pc$values, 3)
```


```{r}
fit.pc$loadings
```
```{r}
# Communalities
fit.pc$communality

```
```{r}
# Rotated factor scores, Notice the columns ordering: RC1, RC2
fit.pc
fit.pc$scores

```
```{r}
fa.plot(fit.pc) # See Correlations within Factors

```


Show the columns that go into each factor?
```{r}
fa.diagram(fit.pc) # Visualize the relationship

```


Perform some visualizations using the factors

```{r}
#very simple structure visualization
vss(sm1)

```
Very Simple Structure
Call: vss(x = sm1)
VSS complexity 1 achieves a maximimum of 0.61  with  6  factors
VSS complexity 2 achieves a maximimum of 0.78  with  7  factors

The Velicer MAP achieves a minimum of 0.06  with  1  factors 
BIC achieves a minimum of  -53.17  with  1  factors
Sample Size adjusted BIC achieves a minimum of  1.47  with  5  factors

Statistics by number of factors 

```{r}
# Computing Correlation Matrix
corrm.social <- cor(sm1)
corrm.social

```
```{r}
plot(corrm.social)

```
```{r}
social_pca <- prcomp(sm1, scale=TRUE)
summary(social_pca)

```


```{r}
plot(social_pca)

```

#Biplot
```{r}
biplot(fit.pc)

```
# Cluster Analysis

```{r}
library(MASS)
library(factoextra)
library(ggplot2)
library(readxl)
library(factoextra)
library(ggfortify)
library(ggrepel)
library(stats)

# Load required library
library(readxl)

# Define the file path
file_path <- "C:/Users/sakad/Downloads/social_media_cleaned.xlsx"

# Read the Excel file
body <- read_excel(file_path)

# Standardize the dataset excluding the first variable which is categorical
data.scaled <- scale(x = body[, -1], center = TRUE, scale = TRUE)

# Assign the standardized data to 'data'
data <- data.scaled

# Display the first few rows of 'data'
head(data)

```



Q: What is the proportion of variance explained by the first three principal components in the PCA analysis, and how does it impact the effectiveness of the subsequent K-means clustering algorithm in identifying distinct clusters within the data?

```{r}
# Perform PCA
pc <- prcomp(data.scaled)

# Extract the first three principal components
pc_first_three <- pc$x[, 1:3]

# Perform K-means clustering on the first three principal components
set.seed(123)  # For reproducibility
k <- 3  # Number of clusters
km_clusters <- kmeans(pc_first_three, centers = k)

# Define colors for each cluster
cluster_colors <- c("red", "blue", "green")

# Plot the first three principal components with cluster assignments
plot(pc_first_three, col = cluster_colors[km_clusters$cluster], 
     main = "First Three Principal Components with Cluster Assignments", 
     xlab = "", ylab = "", pch = 20)

```


This above code first performs Principal Component Analysis (PCA) on scaled data, reducing its dimensionality. Then, it extracts the first three principal components. Next, it applies K-means clustering to these components, dividing data into three clusters. Finally, it plots the first three principal components with color-coded cluster assignments for visualization and analysis.








Q: Can hierarchical clustering based on the first three principal components effectively reveal underlying patterns or groupings within the dataset, and how do the relationships between samples, as depicted in the dendrogram, reflect the distances in the reduced-dimensional space?
```{r}

# Perform PCA
pc <- prcomp(data.scaled)

# Extract the first three principal components
pc_first_three <- pc$x[, 1:3]

# Take a subset of 20 rows
data_subset <- data[1:20, ]

# Perform PCA
pca_result <- prcomp(data_subset)

# Extract the first three principal components
pc_first_three <- pca_result$x[, 1:3]

# Perform hierarchical clustering on the first three principal components
hc <- hclust(dist(pc_first_three))

# Plot the dendrogram
plot(hc, main = "Dendrogram of Hierarchical Clustering (Subset of 20 Rows)",
     xlab = "Sample Index", ylab = "Distance", sub = NULL)
```

The plot shows the first three principal components, performs hierarchical clustering on them, and plots a dendrogram showing the relationships between the samples based on their distances in the reduced-dimensional space.


Q: What is the degree of separation or distinctiveness among the identified clusters when analyzing the distribution of data points in the two-dimensional space formed by the first two Principal Components?
```{r}
# Visualize cluster and membership using first two Principal Components
fviz_cluster(list(data = pc$x[, 1:2], cluster = km_clusters$cluster))
```

This plot visualizes clustering results by plotting data points in a two-dimensional space using the first two Principal Components. Each point is colored according to its assigned cluster, showing the grouping pattern identified by the clustering algorithm. It helps understand how data points are grouped based on their features.




```{r}
# Non-hierarchical clustering (k-means)
num_clusters <- 2  
kmeans_model <- kmeans(data, centers = num_clusters)

# Membership for each cluster
table(kmeans_model$cluster)
```

This represents clustering using the k-means algorithm, dividing data into two clusters. It initializes cluster centers randomly, assigning each data point to the nearest cluster. The table function counts the number of data points assigned to each cluster, providing insight into cluster membership and distribution.





Q: What is the distribution of cluster sizes obtained through k-means clustering when applied to the dataset using the first two principal components, and how does it compare to the distribution of cluster sizes obtained from clustering in the original feature space?
```{r}
# Visualize cluster and membership using first two Principal Components
fviz_cluster(list(data = pc$x[, 1:2], cluster = kmeans_model$cluster))
```

This plot visualizes clusters and their memberships using the first two principal components. It extracts these components from the data, then assigns each data point to a cluster using k-means clustering. Finally, it creates a visual representation showing how the data points are grouped based on their similarities in the first two principal components.





Q: What is the relationship between the clustering results obtained through k-means algorithm and the underlying structure of the data as revealed by Principal Component Analysis (PCA)?
```{r}
# Visualize cluster and membership using first two Principal Components for k-means
pca_result <- prcomp(data, scale = TRUE)
fviz_cluster(kmeans_model, data = pca_result$x[, 1:2], geom = "point", 
             pointsize = 2, fill = "white", main = "K-means Clustering Result (PCA)")
```
 
 This shows visualization of the clusters and their memberships using the first two Principal Components (PCs) obtained from the PCA (Principal Component Analysis) of the numerical data. First, it computes the PCA result for the numerical data and scales it. Then, it uses the `fviz_cluster` function to plot the clusters obtained from the k-means algorithm (`kmeans_model`). It represents each data point as a point on the plot, with the size set to 2 and colored white. The plot is titled "K-means Clustering Result (PCA)". This visualization helps to understand how the data points are grouped into clusters based on their similarities, as revealed by the PCA analysis.






Q:What is the relationship between the number of clusters (k) and the average silhouette width in k-means clustering, and how does this relationship inform the determination of the optimal number of clusters for a given dataset?
```{r}
library(factoextra)
library(cluster)

# Calculate silhouette information for k-means clustering
sil <- silhouette(kmeans_model$cluster, dist(data))

# Visualize the silhouette plot for k-means clustering
fviz_silhouette(sil, main = "Silhouette Plot for K-means Clustering")

```

This plot calculates and visualizes the silhouette information for k-means clustering. Silhouette analysis helps evaluate the quality of clustering by measuring how similar an object is to its own cluster compared to other clusters. A higher silhouette width indicates better separation of clusters, while negative values suggest that points might be assigned to the wrong clusters. This plot helps in determining the optimal number of clusters for k-means clustering and assessing the overall clustering performance.






Q: Is there a significant difference in the clustering patterns based on Linkedin and Youtube among different groups represented by distinct colors on the plot?
```{r}
# Create a data frame with cluster membership
data_clustered <- data.frame(data, Cluster = kmeans_model$cluster)  # Ensure conversion to data frame

# Scatter plot of data points colored by cluster membership
plot(data_clustered$LinkedIn, data_clustered$youtube, 
     col = data_clustered$Cluster, pch = 16, 
     xlab = "Linkedin", ylab = "youtube",  
     main = "Scatter Plot of Clustering")
legend("topright", legend = unique(data_clustered$Cluster), 
       col = 1:max(data_clustered$Cluster), pch = 16, title = "Cluster")

```

Overall, this plot visualizes clusters in the data, helping us understand
how data points group together based on the Linkedin and Youtube, with each group 
represented by a different color on the plot.










